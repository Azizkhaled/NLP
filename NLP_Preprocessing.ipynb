{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "If running in google colab, you can upload the requirements.txt from the following cell"
      ],
      "metadata": {
        "id": "OFae6D3uzgMg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "5NqwcrNQie7s",
        "outputId": "26896754-4463-40a8-aa3c-812e92e98b21"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0b8bd695-e02e-4c3b-96bf-17a92edda538\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0b8bd695-e02e-4c3b-96bf-17a92edda538\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving requirements.txt to requirements.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0, nb_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdjkPJIui2i0",
        "outputId": "1df9c5d1-ab4f-468e-ce2b-b8beae4c289a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --target=$nb_path -r requirements.txt\n"
      ],
      "metadata": {
        "id": "p-BM2vzXi_Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP Preprocessing"
      ],
      "metadata": {
        "id": "5Xy9HkUvklW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Stopwords"
      ],
      "metadata": {
        "id": "iI1kamPkkqNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = \"\"\"I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
        "\n",
        "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\""
      ],
      "metadata": {
        "id": "UknNZ3LPjO3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU-A6adBk9S0",
        "outputId": "b9bda63c-f813-4fdd-d995-0fe4d1d50768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "zbfr3OuFlAQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stop_words)"
      ],
      "metadata": {
        "id": "covvB9ynlNxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = tweet.lower().split()"
      ],
      "metadata": {
        "id": "MYa94VpFmGJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_no_stopwords = [word for word in tweet if word not in stop_words]\n",
        "\n",
        "print(\"With stopwords:\", ' '.join(tweet))\n",
        "print(\"Without:\", ' '.join(tweet_no_stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIvi2D89mH7Q",
        "outputId": "94aebe9e-861b-40f4-b086-5de1595be634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With stopwords: i’m amazed how often in practice, not only does a @huggingface nlp model solve your problem, but one of their public finetuned checkpoints, is good enough for the job. both impressed, and a little disappointed how rarely i get to actually train a model that matters :(\n",
            "Without: i’m amazed often practice, @huggingface nlp model solve problem, one public finetuned checkpoints, good enough job. impressed, little disappointed rarely get actually train model matters :(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokens"
      ],
      "metadata": {
        "id": "L5NHVNOcmP5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A word\n",
        "* Part of a word\n",
        "* A single character\n",
        "* Puntuation mark [,!-.]\n",
        "* Special token like < URL >, or < NAME >\n",
        "* Model-specific special tokens, like [CLS] and [SEP] for BERT"
      ],
      "metadata": {
        "id": "iQhowrKhoho8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the BERT transformer model there are *five* special tokens that are used by the model, these are:\n",
        "\n",
        "| Token | Meaning |\n",
        "| --- | --- |\n",
        "| **[PAD]** | Padding token, allows us to maintain same-length sequences (512 tokens for Bert) even when different sized sentences are fed in |\n",
        "| **[UNK]** | Used when a word is unknown to Bert |\n",
        "| **[CLS]** | Appears at the start of every sequence |\n",
        "| **[SEP]** | Indicates a seperator or end of sequence |\n",
        "| **[MASK]** | Used when masking tokens, for example in training with masked language modelling (MLM) |"
      ],
      "metadata": {
        "id": "v_57zgpdqehZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "28SIzxharESv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is a text normalization method used in NLP to simplify text before it is processed by a model. When stemming break the final few characters of a word in order to find a common form of the word."
      ],
      "metadata": {
        "id": "qNyRJOaxraCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"I am amazed by how amazingly amazing you are\""
      ],
      "metadata": {
        "id": "L8XRvR8SmJzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_to_stem = ['happy', 'happiest', 'happier', 'cactus', 'cactii', 'elephant', 'elephants', 'amazed', 'amazing', 'amazingly', 'cement', 'owed', 'maximum', 'maxim']"
      ],
      "metadata": {
        "id": "GcDA8Z5wrOQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer"
      ],
      "metadata": {
        "id": "fxgZju0SrfZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()"
      ],
      "metadata": {
        "id": "_MW1C32xrqrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed = [(word, porter.stem(word), lancaster.stem(word)) for word in words_to_stem]"
      ],
      "metadata": {
        "id": "iX0dbJsYrsjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W1idCbjruGD",
        "outputId": "8bede5cc-1f27-4c5e-91ec-1e09f9af7621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('happy', 'happi', 'happy'),\n",
              " ('happiest', 'happiest', 'happiest'),\n",
              " ('happier', 'happier', 'happy'),\n",
              " ('cactus', 'cactu', 'cact'),\n",
              " ('cactii', 'cactii', 'cacti'),\n",
              " ('elephant', 'eleph', 'eleph'),\n",
              " ('elephants', 'eleph', 'eleph'),\n",
              " ('amazed', 'amaz', 'amaz'),\n",
              " ('amazing', 'amaz', 'amaz'),\n",
              " ('amazingly', 'amazingli', 'amaz'),\n",
              " ('cement', 'cement', 'cem'),\n",
              " ('owed', 'owe', 'ow'),\n",
              " ('maximum', 'maximum', 'maxim'),\n",
              " ('maxim', 'maxim', 'maxim')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "\n",
        "Lemmatization is very similiar to stemming in that it reduces a set of inflected words down to a common word. The difference is that lemmatization reduces inflections down to their real root words, which is called a lemma"
      ],
      "metadata": {
        "id": "GvBOKiTFseQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['amaze', 'amazed', 'amazing']\n"
      ],
      "metadata": {
        "id": "lh8PUNwrr5wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSOaaMGnsvZk",
        "outputId": "ebd6fee7-5b2d-4a48-833d-029149387fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "jbARBqX8sxDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[lemmatizer.lemmatize(word) for word in words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo8FSnmIs0HK",
        "outputId": "be207299-2d5f-4034-834a-2b6ecae40056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['amaze', 'amazed', 'amazing']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we could place each word as a verb, which we can then implement like so:"
      ],
      "metadata": {
        "id": "9TCC1pOQs9Oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[lemmatizer.lemmatize(word, wordnet.VERB) for word in words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji7DtU65s145",
        "outputId": "a1359d1a-41d9-43d5-9e24-0f54b191a2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['amaze', 'amaze', 'amaze']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unicode Normalization"
      ],
      "metadata": {
        "id": "SyS1rI74tKeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata"
      ],
      "metadata": {
        "id": "sfl4Gz6GwEXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use unicode normalization to *normalize* our characters into matching pairs. As there are different forms of equivalence, there are also different forms of normalization. These are all called **N**ormal **F**orm, and there are four different methods:\n",
        "\n",
        "| Name | Abbreviation | Description | Example |\n",
        "| --- | --- | --- | --- |\n",
        "| Form D | NFD | *Canonical* decomposition | `Ç` → `C ̧` |\n",
        "| Form C | NFC | *Canoncial* decomposition followed by *canonical* composition | `Ç` → `C ̧` → `Ç` |\n",
        "| Form KD | NFKD | *Compatibility* decomposition | `ℌ ̧` → `H ̧` |\n",
        "| Form KC | NFKC | *Compatibility* decomposition followed by *canonical* composition | `ℌ ̧` → `H ̧` → `Ḩ` |\n",
        "\n",
        "Let's take a look at each of these forms in action. Our C with cedilla character Ç can be represented in two ways, as a single character called *Latin capital C with cedilla* (*\\u00C7*), or as two characte"
      ],
      "metadata": {
        "id": "ldtgBdTAuEH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_with_cedilla = \"\\u00C7\"  # Latin capital C with cedilla (single character)\n",
        "c_with_cedilla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MHab1zRAs_Qx",
        "outputId": "11b93a2f-570a-4471-9813-0802ccd3f544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ç'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c_plus_cedilla = \"\\u0043\\u0327\"  # \\u0043 = Latin capital C, \\u0327 = 'combining cedilla' (two characters)\n",
        "c_plus_cedilla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RjiWZAPvvf-S",
        "outputId": "34f8ac97-099a-4f3f-d134-34eda33fced5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ç'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#And we will find that these two version do not match when compared:\n",
        "c_with_cedilla == c_plus_cedilla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu8ox_Elv355",
        "outputId": "a8ad1eb8-f59e-4484-bbcf-19e5beb16eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we perform **NFD** on our C with cedilla character `\\u00C7`, we **decompose** the character into it's smaller components, which are the *Latin capital C* character, and *combining cedilla* character `\\u0043` + `\\u0327`. This means that if we compare an **NFD** normalized C with cedilla character to both the C character and the cedilla character, we will return true:"
      ],
      "metadata": {
        "id": "Qxnz-fkpwZwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unicodedata.normalize('NFD', c_with_cedilla) == c_plus_cedilla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EuVoNxpv5QJ",
        "outputId": "b0f76c79-0814-4891-f100-cb4f8f9f98fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But if we switch the **NFC** encoding to instead be performed on our two characters `\\u0043` + `\\u0327`, they will first be **decomposed** (which will do nothing as they are already decomposed), then compose them into the single `\\u00C7` character:"
      ],
      "metadata": {
        "id": "RgZLWIxnwn6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_with_cedilla == unicodedata.normalize('NFC', c_plus_cedilla)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5GwfRrDwGjl",
        "outputId": "c1406c75-dc6a-4db0-e610-77d0d80c0235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unicodedata.normalize('NFKC', 'ℌ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hSW0h__Uwl46",
        "outputId": "467283f5-69b3-488c-c6a4-41702aac71eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'H'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ]
}